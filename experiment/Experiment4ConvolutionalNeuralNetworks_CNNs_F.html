<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=Su9cPBdaKucZGyXzr7BkXw);.lst-kix_list_14-1>li:before{content:"\0025cb   "}.lst-kix_list_14-3>li:before{content:"\0025cf   "}ul.lst-kix_list_1-0{list-style-type:none}ol.lst-kix_list_12-6.start{counter-reset:lst-ctn-kix_list_12-6 0}.lst-kix_list_14-0>li:before{content:"\0025cf   "}.lst-kix_list_14-4>li:before{content:"\0025cb   "}.lst-kix_list_14-5>li:before{content:"\0025a0   "}.lst-kix_list_14-7>li:before{content:"\0025cb   "}ol.lst-kix_list_13-4.start{counter-reset:lst-ctn-kix_list_13-4 0}.lst-kix_list_14-6>li:before{content:"\0025cf   "}ul.lst-kix_list_9-3{list-style-type:none}ul.lst-kix_list_9-4{list-style-type:none}ul.lst-kix_list_9-1{list-style-type:none}ul.lst-kix_list_9-2{list-style-type:none}ul.lst-kix_list_9-7{list-style-type:none}.lst-kix_list_13-0>li{counter-increment:lst-ctn-kix_list_13-0}ul.lst-kix_list_9-8{list-style-type:none}ul.lst-kix_list_9-5{list-style-type:none}.lst-kix_list_5-0>li{counter-increment:lst-ctn-kix_list_5-0}ul.lst-kix_list_9-6{list-style-type:none}ul.lst-kix_list_1-3{list-style-type:none}ul.lst-kix_list_1-4{list-style-type:none}ul.lst-kix_list_1-1{list-style-type:none}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}ul.lst-kix_list_9-0{list-style-type:none}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}.lst-kix_list_14-2>li:before{content:"\0025a0   "}ul.lst-kix_list_1-6{list-style-type:none}ol.lst-kix_list_5-3.start{counter-reset:lst-ctn-kix_list_5-3 0}ol.lst-kix_list_12-0.start{counter-reset:lst-ctn-kix_list_12-0 0}.lst-kix_list_14-8>li:before{content:"\0025a0   "}.lst-kix_list_5-2>li{counter-increment:lst-ctn-kix_list_5-2}.lst-kix_list_5-0>li:before{content:"" counter(lst-ctn-kix_list_5-0,decimal) ". "}.lst-kix_list_5-4>li{counter-increment:lst-ctn-kix_list_5-4}.lst-kix_list_5-3>li:before{content:"" counter(lst-ctn-kix_list_5-3,decimal) ". "}.lst-kix_list_13-2>li{counter-increment:lst-ctn-kix_list_13-2}.lst-kix_list_5-2>li:before{content:"" counter(lst-ctn-kix_list_5-2,lower-roman) ". "}.lst-kix_list_5-1>li:before{content:"" counter(lst-ctn-kix_list_5-1,lower-latin) ". "}li.li-bullet-6:before{margin-left:-12.2pt;white-space:nowrap;display:inline-block;min-width:12.2pt}.lst-kix_list_5-7>li:before{content:"" counter(lst-ctn-kix_list_5-7,lower-latin) ". "}ul.lst-kix_list_8-4{list-style-type:none}ul.lst-kix_list_8-5{list-style-type:none}.lst-kix_list_5-6>li:before{content:"" counter(lst-ctn-kix_list_5-6,decimal) ". "}.lst-kix_list_5-8>li:before{content:"" counter(lst-ctn-kix_list_5-8,lower-roman) ". "}ul.lst-kix_list_8-2{list-style-type:none}ul.lst-kix_list_8-3{list-style-type:none}ul.lst-kix_list_8-8{list-style-type:none}ul.lst-kix_list_8-6{list-style-type:none}ul.lst-kix_list_8-7{list-style-type:none}.lst-kix_list_5-4>li:before{content:"" counter(lst-ctn-kix_list_5-4,lower-latin) ". "}.lst-kix_list_5-5>li:before{content:"" counter(lst-ctn-kix_list_5-5,lower-roman) ". "}ul.lst-kix_list_8-0{list-style-type:none}ul.lst-kix_list_8-1{list-style-type:none}.lst-kix_list_12-1>li{counter-increment:lst-ctn-kix_list_12-1}ol.lst-kix_list_12-5.start{counter-reset:lst-ctn-kix_list_12-5 0}.lst-kix_list_6-1>li:before{content:"o  "}.lst-kix_list_6-3>li:before{content:"\0025cf   "}ol.lst-kix_list_13-3.start{counter-reset:lst-ctn-kix_list_13-3 0}.lst-kix_list_6-0>li:before{content:"\0025cf   "}.lst-kix_list_6-4>li:before{content:"o  "}li.li-bullet-2:before{margin-left:-12.1pt;white-space:nowrap;display:inline-block;min-width:12.1pt}.lst-kix_list_6-2>li:before{content:"\0025aa   "}.lst-kix_list_6-8>li:before{content:"\0025aa   "}.lst-kix_list_6-5>li:before{content:"\0025aa   "}.lst-kix_list_6-7>li:before{content:"o  "}.lst-kix_list_6-6>li:before{content:"\0025cf   "}.lst-kix_list_2-7>li:before{content:"o  "}.lst-kix_list_7-4>li:before{content:"o  "}.lst-kix_list_7-6>li:before{content:"\0025cf   "}.lst-kix_list_2-5>li:before{content:"\0025aa   "}.lst-kix_list_7-2>li:before{content:"\0025aa   "}ul.lst-kix_list_3-7{list-style-type:none}.lst-kix_list_12-6>li{counter-increment:lst-ctn-kix_list_12-6}ul.lst-kix_list_3-8{list-style-type:none}.lst-kix_list_10-1>li:before{content:"o  "}.lst-kix_list_13-7>li:before{content:"(" counter(lst-ctn-kix_list_13-7,lower-latin) ") "}ul.lst-kix_list_3-1{list-style-type:none}ul.lst-kix_list_3-2{list-style-type:none}.lst-kix_list_5-7>li{counter-increment:lst-ctn-kix_list_5-7}.lst-kix_list_7-8>li:before{content:"\0025aa   "}ul.lst-kix_list_3-0{list-style-type:none}ul.lst-kix_list_3-5{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}.lst-kix_list_10-7>li:before{content:"o  "}.lst-kix_list_15-5>li:before{content:"\0025a0   "}.lst-kix_list_10-5>li:before{content:"\0025aa   "}.lst-kix_list_13-4>li{counter-increment:lst-ctn-kix_list_13-4}ol.lst-kix_list_13-5.start{counter-reset:lst-ctn-kix_list_13-5 0}li.li-bullet-1:before{margin-left:-12.1pt;white-space:nowrap;display:inline-block;min-width:12.1pt}.lst-kix_list_10-3>li:before{content:"\0025cf   "}ul.lst-kix_list_11-7{list-style-type:none}ul.lst-kix_list_11-6{list-style-type:none}.lst-kix_list_4-1>li:before{content:"o  "}ul.lst-kix_list_11-5{list-style-type:none}ul.lst-kix_list_11-4{list-style-type:none}ul.lst-kix_list_11-3{list-style-type:none}ol.lst-kix_list_13-8.start{counter-reset:lst-ctn-kix_list_13-8 0}.lst-kix_list_15-7>li:before{content:"\0025cb   "}ul.lst-kix_list_11-2{list-style-type:none}ul.lst-kix_list_11-1{list-style-type:none}ul.lst-kix_list_11-0{list-style-type:none}.lst-kix_list_9-2>li:before{content:"\0025aa   "}.lst-kix_list_4-3>li:before{content:"\0025cf   "}.lst-kix_list_4-5>li:before{content:"\0025aa   "}ol.lst-kix_list_5-7.start{counter-reset:lst-ctn-kix_list_5-7 0}ul.lst-kix_list_11-8{list-style-type:none}.lst-kix_list_12-5>li{counter-increment:lst-ctn-kix_list_12-5}.lst-kix_list_15-1>li:before{content:"\0025cb   "}.lst-kix_list_5-5>li{counter-increment:lst-ctn-kix_list_5-5}.lst-kix_list_9-0>li:before{content:"\0025cf   "}.lst-kix_list_15-3>li:before{content:"\0025cf   "}.lst-kix_list_9-6>li:before{content:"\0025cf   "}li.li-bullet-9:before{margin-left:-12.1pt;white-space:nowrap;display:inline-block;min-width:12.1pt}.lst-kix_list_9-4>li:before{content:"o  "}.lst-kix_list_11-3>li:before{content:"\0025cf   "}ul.lst-kix_list_2-8{list-style-type:none}ol.lst-kix_list_12-1.start{counter-reset:lst-ctn-kix_list_12-1 0}.lst-kix_list_12-3>li:before{content:"" counter(lst-ctn-kix_list_12-3,decimal) ". "}.lst-kix_list_11-5>li:before{content:"\0025aa   "}.lst-kix_list_12-1>li:before{content:"" counter(lst-ctn-kix_list_12-1,lower-latin) ". "}ul.lst-kix_list_2-2{list-style-type:none}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-0{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}.lst-kix_list_9-8>li:before{content:"\0025aa   "}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:"o  "}ul.lst-kix_list_2-7{list-style-type:none}.lst-kix_list_11-7>li:before{content:"o  "}.lst-kix_list_13-3>li{counter-increment:lst-ctn-kix_list_13-3}ol.lst-kix_list_13-6.start{counter-reset:lst-ctn-kix_list_13-6 0}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}ul.lst-kix_list_10-0{list-style-type:none}.lst-kix_list_1-3>li:before{content:"\0025cf   "}.lst-kix_list_13-3>li:before{content:"" counter(lst-ctn-kix_list_13-3,lower-latin) ") "}ul.lst-kix_list_10-8{list-style-type:none}ul.lst-kix_list_10-7{list-style-type:none}.lst-kix_list_1-7>li:before{content:"o  "}ol.lst-kix_list_5-8.start{counter-reset:lst-ctn-kix_list_5-8 0}ul.lst-kix_list_10-6{list-style-type:none}ul.lst-kix_list_10-5{list-style-type:none}ul.lst-kix_list_10-4{list-style-type:none}ul.lst-kix_list_10-3{list-style-type:none}.lst-kix_list_1-5>li:before{content:"\0025aa   "}li.li-bullet-4:before{margin-left:-16.5pt;white-space:nowrap;display:inline-block;min-width:16.5pt}ul.lst-kix_list_10-2{list-style-type:none}ul.lst-kix_list_10-1{list-style-type:none}.lst-kix_list_13-5>li:before{content:"(" counter(lst-ctn-kix_list_13-5,lower-latin) ") "}.lst-kix_list_5-6>li{counter-increment:lst-ctn-kix_list_5-6}.lst-kix_list_12-5>li:before{content:"" counter(lst-ctn-kix_list_12-5,lower-roman) ". "}ol.lst-kix_list_13-7.start{counter-reset:lst-ctn-kix_list_13-7 0}ol.lst-kix_list_12-2.start{counter-reset:lst-ctn-kix_list_12-2 0}.lst-kix_list_12-7>li:before{content:"" counter(lst-ctn-kix_list_12-7,lower-latin) ". "}.lst-kix_list_2-1>li:before{content:"o  "}.lst-kix_list_2-3>li:before{content:"\0025cf   "}.lst-kix_list_13-1>li:before{content:"" counter(lst-ctn-kix_list_13-1,upper-latin) ". "}.lst-kix_list_5-1>li{counter-increment:lst-ctn-kix_list_5-1}.lst-kix_list_3-0>li:before{content:"\0025cf   "}ol.lst-kix_list_13-1.start{counter-reset:lst-ctn-kix_list_13-1 0}.lst-kix_list_3-1>li:before{content:"o  "}.lst-kix_list_3-2>li:before{content:"\0025aa   "}.lst-kix_list_8-1>li:before{content:"o  "}.lst-kix_list_8-2>li:before{content:"\0025aa   "}.lst-kix_list_3-5>li:before{content:"\0025aa   "}.lst-kix_list_12-0>li{counter-increment:lst-ctn-kix_list_12-0}.lst-kix_list_3-4>li:before{content:"o  "}ol.lst-kix_list_12-3.start{counter-reset:lst-ctn-kix_list_12-3 0}.lst-kix_list_3-3>li:before{content:"\0025cf   "}.lst-kix_list_8-0>li:before{content:"\0025cf   "}.lst-kix_list_8-7>li:before{content:"o  "}.lst-kix_list_3-8>li:before{content:"\0025aa   "}.lst-kix_list_8-5>li:before{content:"\0025aa   "}.lst-kix_list_8-6>li:before{content:"\0025cf   "}.lst-kix_list_13-1>li{counter-increment:lst-ctn-kix_list_13-1}.lst-kix_list_8-3>li:before{content:"\0025cf   "}.lst-kix_list_3-6>li:before{content:"\0025cf   "}.lst-kix_list_3-7>li:before{content:"o  "}.lst-kix_list_8-4>li:before{content:"o  "}ol.lst-kix_list_5-0.start{counter-reset:lst-ctn-kix_list_5-0 0}.lst-kix_list_11-2>li:before{content:"\0025aa   "}.lst-kix_list_11-1>li:before{content:"o  "}.lst-kix_list_11-0>li:before{content:"\0025cf   "}.lst-kix_list_8-8>li:before{content:"\0025aa   "}ol.lst-kix_list_12-4.start{counter-reset:lst-ctn-kix_list_12-4 0}.lst-kix_list_4-8>li:before{content:"\0025aa   "}ol.lst-kix_list_12-5{list-style-type:none}ol.lst-kix_list_12-6{list-style-type:none}.lst-kix_list_4-7>li:before{content:"o  "}ol.lst-kix_list_12-7{list-style-type:none}ol.lst-kix_list_12-8{list-style-type:none}ol.lst-kix_list_12-1{list-style-type:none}li.li-bullet-7:before{margin-left:-12.2pt;white-space:nowrap;display:inline-block;min-width:12.2pt}ol.lst-kix_list_12-2{list-style-type:none}ol.lst-kix_list_12-3{list-style-type:none}ol.lst-kix_list_12-4{list-style-type:none}ul.lst-kix_list_4-8{list-style-type:none}ol.lst-kix_list_5-6.start{counter-reset:lst-ctn-kix_list_5-6 0}ul.lst-kix_list_4-6{list-style-type:none}ul.lst-kix_list_4-7{list-style-type:none}ol.lst-kix_list_12-0{list-style-type:none}ul.lst-kix_list_4-0{list-style-type:none}ul.lst-kix_list_4-1{list-style-type:none}ul.lst-kix_list_4-4{list-style-type:none}ul.lst-kix_list_4-5{list-style-type:none}ul.lst-kix_list_4-2{list-style-type:none}ul.lst-kix_list_4-3{list-style-type:none}.lst-kix_list_12-4>li{counter-increment:lst-ctn-kix_list_12-4}.lst-kix_list_12-7>li{counter-increment:lst-ctn-kix_list_12-7}ol.lst-kix_list_5-5.start{counter-reset:lst-ctn-kix_list_5-5 0}.lst-kix_list_7-0>li:before{content:"\0025cf   "}.lst-kix_list_13-8>li{counter-increment:lst-ctn-kix_list_13-8}ol.lst-kix_list_5-0{list-style-type:none}.lst-kix_list_2-6>li:before{content:"\0025cf   "}ol.lst-kix_list_5-1{list-style-type:none}ol.lst-kix_list_5-2{list-style-type:none}ol.lst-kix_list_13-8{list-style-type:none}.lst-kix_list_2-4>li:before{content:"o  "}.lst-kix_list_2-8>li:before{content:"\0025aa   "}.lst-kix_list_7-1>li:before{content:"o  "}.lst-kix_list_7-5>li:before{content:"\0025aa   "}.lst-kix_list_13-5>li{counter-increment:lst-ctn-kix_list_13-5}ol.lst-kix_list_13-4{list-style-type:none}ol.lst-kix_list_13-5{list-style-type:none}ol.lst-kix_list_13-6{list-style-type:none}ol.lst-kix_list_13-7{list-style-type:none}ol.lst-kix_list_13-0{list-style-type:none}ol.lst-kix_list_13-1{list-style-type:none}ol.lst-kix_list_13-2{list-style-type:none}ol.lst-kix_list_5-4.start{counter-reset:lst-ctn-kix_list_5-4 0}.lst-kix_list_7-3>li:before{content:"\0025cf   "}ol.lst-kix_list_13-3{list-style-type:none}ul.lst-kix_list_7-5{list-style-type:none}.lst-kix_list_10-0>li:before{content:"\0025cf   "}ul.lst-kix_list_7-6{list-style-type:none}ul.lst-kix_list_7-3{list-style-type:none}ul.lst-kix_list_7-4{list-style-type:none}.lst-kix_list_13-6>li:before{content:"(" counter(lst-ctn-kix_list_13-6,lower-roman) ") "}.lst-kix_list_13-6>li{counter-increment:lst-ctn-kix_list_13-6}.lst-kix_list_13-8>li:before{content:"(" counter(lst-ctn-kix_list_13-8,lower-roman) ") "}ol.lst-kix_list_5-1.start{counter-reset:lst-ctn-kix_list_5-1 0}ul.lst-kix_list_7-7{list-style-type:none}ul.lst-kix_list_7-8{list-style-type:none}ol.lst-kix_list_5-7{list-style-type:none}ol.lst-kix_list_5-8{list-style-type:none}ol.lst-kix_list_5-3{list-style-type:none}ul.lst-kix_list_7-1{list-style-type:none}ol.lst-kix_list_5-4{list-style-type:none}ul.lst-kix_list_7-2{list-style-type:none}ol.lst-kix_list_5-5{list-style-type:none}ol.lst-kix_list_5-6{list-style-type:none}ul.lst-kix_list_7-0{list-style-type:none}.lst-kix_list_7-7>li:before{content:"o  "}.lst-kix_list_15-4>li:before{content:"\0025cb   "}.lst-kix_list_15-6>li:before{content:"\0025cf   "}.lst-kix_list_5-8>li{counter-increment:lst-ctn-kix_list_5-8}.lst-kix_list_10-4>li:before{content:"o  "}.lst-kix_list_10-8>li:before{content:"\0025aa   "}.lst-kix_list_4-0>li:before{content:"\0025cf   "}ul.lst-kix_list_15-3{list-style-type:none}ul.lst-kix_list_15-2{list-style-type:none}.lst-kix_list_15-0>li:before{content:"\0025cf   "}ul.lst-kix_list_15-1{list-style-type:none}.lst-kix_list_15-8>li:before{content:"\0025a0   "}ul.lst-kix_list_15-0{list-style-type:none}li.li-bullet-3:before{margin-left:-16.5pt;white-space:nowrap;display:inline-block;min-width:16.5pt}.lst-kix_list_10-2>li:before{content:"\0025aa   "}.lst-kix_list_13-7>li{counter-increment:lst-ctn-kix_list_13-7}.lst-kix_list_4-4>li:before{content:"o  "}ul.lst-kix_list_15-8{list-style-type:none}.lst-kix_list_4-2>li:before{content:"\0025aa   "}.lst-kix_list_4-6>li:before{content:"\0025cf   "}ul.lst-kix_list_15-7{list-style-type:none}ul.lst-kix_list_15-6{list-style-type:none}.lst-kix_list_9-3>li:before{content:"\0025cf   "}ul.lst-kix_list_15-5{list-style-type:none}ul.lst-kix_list_15-4{list-style-type:none}.lst-kix_list_15-2>li:before{content:"\0025a0   "}.lst-kix_list_12-8>li{counter-increment:lst-ctn-kix_list_12-8}ol.lst-kix_list_13-2.start{counter-reset:lst-ctn-kix_list_13-2 0}.lst-kix_list_10-6>li:before{content:"\0025cf   "}.lst-kix_list_9-1>li:before{content:"o  "}ol.lst-kix_list_12-7.start{counter-reset:lst-ctn-kix_list_12-7 0}.lst-kix_list_9-7>li:before{content:"o  "}li.li-bullet-8:before{margin-left:-12.2pt;white-space:nowrap;display:inline-block;min-width:12.2pt}.lst-kix_list_12-2>li{counter-increment:lst-ctn-kix_list_12-2}.lst-kix_list_11-4>li:before{content:"o  "}.lst-kix_list_12-4>li:before{content:"" counter(lst-ctn-kix_list_12-4,lower-latin) ". "}.lst-kix_list_9-5>li:before{content:"\0025aa   "}ul.lst-kix_list_6-6{list-style-type:none}li.li-bullet-5:before{margin-left:-16.5pt;white-space:nowrap;display:inline-block;min-width:16.5pt}ul.lst-kix_list_6-7{list-style-type:none}.lst-kix_list_5-3>li{counter-increment:lst-ctn-kix_list_5-3}ul.lst-kix_list_6-4{list-style-type:none}ul.lst-kix_list_6-5{list-style-type:none}ul.lst-kix_list_6-8{list-style-type:none}.lst-kix_list_12-2>li:before{content:"" counter(lst-ctn-kix_list_12-2,lower-roman) ". "}ol.lst-kix_list_12-8.start{counter-reset:lst-ctn-kix_list_12-8 0}.lst-kix_list_11-6>li:before{content:"\0025cf   "}.lst-kix_list_1-0>li:before{content:"\0025cf   "}ul.lst-kix_list_6-2{list-style-type:none}.lst-kix_list_11-8>li:before{content:"\0025aa   "}.lst-kix_list_12-3>li{counter-increment:lst-ctn-kix_list_12-3}ul.lst-kix_list_6-3{list-style-type:none}.lst-kix_list_1-2>li:before{content:"\0025aa   "}ul.lst-kix_list_6-0{list-style-type:none}.lst-kix_list_12-0>li:before{content:"" counter(lst-ctn-kix_list_12-0,decimal) ". "}ul.lst-kix_list_6-1{list-style-type:none}.lst-kix_list_1-4>li:before{content:"o  "}.lst-kix_list_13-0>li:before{content:"" counter(lst-ctn-kix_list_13-0,upper-roman) ". "}ul.lst-kix_list_14-4{list-style-type:none}ul.lst-kix_list_14-3{list-style-type:none}ol.lst-kix_list_13-0.start{counter-reset:lst-ctn-kix_list_13-0 0}ul.lst-kix_list_14-2{list-style-type:none}.lst-kix_list_13-4>li:before{content:"(" counter(lst-ctn-kix_list_13-4,decimal) ") "}ul.lst-kix_list_14-1{list-style-type:none}ul.lst-kix_list_14-0{list-style-type:none}.lst-kix_list_1-6>li:before{content:"\0025cf   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_list_14-8{list-style-type:none}ul.lst-kix_list_14-7{list-style-type:none}.lst-kix_list_2-0>li:before{content:"\0025cf   "}.lst-kix_list_12-6>li:before{content:"" counter(lst-ctn-kix_list_12-6,decimal) ". "}ul.lst-kix_list_14-6{list-style-type:none}ul.lst-kix_list_14-5{list-style-type:none}.lst-kix_list_1-8>li:before{content:"\0025aa   "}.lst-kix_list_2-2>li:before{content:"\0025aa   "}.lst-kix_list_13-2>li:before{content:"" counter(lst-ctn-kix_list_13-2,decimal) ". "}ol.lst-kix_list_5-2.start{counter-reset:lst-ctn-kix_list_5-2 0}.lst-kix_list_12-8>li:before{content:"" counter(lst-ctn-kix_list_12-8,lower-roman) ". "}ol{margin:0;padding:0}table td,table th{padding:0}.c39{padding-top:0pt;text-indent:36pt;padding-bottom:6pt;line-height:0.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c23{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c3{margin-left:36pt;padding-top:6pt;padding-left:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c24{margin-left:7.1pt;padding-top:6pt;text-indent:-18pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c5{margin-left:28.3pt;padding-top:6pt;text-indent:-18pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c14{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c37{margin-left:22.8pt;padding-top:12pt;padding-left:-1.5pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c21{margin-left:36pt;padding-top:6pt;padding-left:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c7{margin-left:41.9pt;padding-top:12pt;padding-left:-5.9pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c1{margin-left:14.2pt;padding-top:6pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c10{padding-top:6pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left;height:12pt}.c18{padding-top:6pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:center;height:12pt}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c13{color:#1f3763;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c2{margin-left:21.3pt;padding-top:6pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c16{padding-top:6pt;text-indent:14.2pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c11{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c8{padding-top:14.1pt;padding-bottom:7pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c49{padding-top:0pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c25{padding-top:6pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c41{padding-top:6pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c52{padding-top:7pt;padding-bottom:7pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c59{padding-top:14.1pt;padding-bottom:7pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c35{padding-top:0pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c51{padding-top:0pt;padding-bottom:6pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c53{padding-top:6pt;padding-bottom:6pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c26{font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman"}.c9{font-size:17pt;font-family:"Arial";font-style:normal;color:#000000;font-weight:400}.c57{background-color:#ffffff;max-width:447.9pt;padding:72pt 72pt 72pt 92.1pt}.c4{font-size:10pt;font-weight:400;font-family:"Arial"}.c27{font-size:10pt;font-weight:400;font-family:"Arimo"}.c20{font-size:11pt;font-weight:400;font-family:"Arial"}.c62{font-size:13pt;font-weight:400;font-family:"Arial"}.c40{color:#1f3763;font-size:17pt;font-style:normal}.c44{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c17{font-size:10pt;font-weight:700;font-family:"Arial"}.c33{padding:0;margin:0}.c50{margin-left:8.6pt;padding-left:-1.5pt}.c30{margin-left:77.8pt;padding-left:-5.8pt}.c34{margin-left:41.9pt;padding-left:-5.9pt}.c45{margin-left:22.5pt;padding-left:-1.5pt}.c28{text-decoration:none;vertical-align:baseline}.c36{margin-left:28.3pt;padding-left:0pt}.c22{font-weight:700;font-family:"Arial"}.c15{font-style:normal;color:#000000}.c56{margin-left:22.8pt;padding-left:-1.5pt}.c29{margin-left:-7.1pt}.c43{font-style:italic}.c55{margin-left:7.1pt}.c38{padding-left:0pt}.c42{text-indent:-14.2pt}.c48{text-indent:-32.2pt}.c31{margin-left:39.3pt}.c46{margin-left:36pt}.c32{color:#000000}.c54{margin-left:21.3pt}.c47{text-indent:-18pt}.c60{color:#187b34}.c61{text-indent:-36pt}.c19{height:12pt}.c58{text-indent:28.3pt}.c63{margin-left:28.3pt}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.0;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:12pt;font-family:"Times New Roman"}p{margin:0;color:#000000;font-size:12pt;font-family:"Times New Roman"}h1{padding-top:12pt;color:#2f5496;font-weight:700;font-size:24pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:2pt;color:#2f5496;font-weight:700;font-size:18pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:2pt;color:#1f3763;font-weight:700;font-size:14pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:2pt;color:#2f5496;font-weight:700;font-size:12pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:2pt;color:#2f5496;font-weight:700;font-size:10pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:2pt;color:#1f3763;font-weight:700;font-size:8pt;padding-bottom:0pt;font-family:"Times New Roman";line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}</style></head><body class="c57 doc-content"><h3 class="c52"><span class="c9">Experiment-4</span></h3><p class="c10"><span class="c6"></span></p><h3 class="c59"><span class="c9">Convolutional Neural Networks (CNN)</span></h3><p class="c10"><span class="c6"></span></p><p class="c25"><span class="c22">1. Aim</span></p><p class="c10"><span class="c6"></span></p><p class="c25"><span class="c0">To understand the theoretical foundations of Convolutional Neural Networks (CNNs) and their application to image classification, and to study how convolution, feature extraction, and pooling operations enable efficient learning from color images, preparing students for practical implementation on the CIFAR-10 dataset.</span></p><p class="c10"><span class="c0"></span></p><p class="c25"><span class="c22">2. Theory</span></p><p class="c10"><span class="c6"></span></p><ol class="c33 lst-kix_list_13-0 start" start="1"><li class="c25 c36 li-bullet-0"><span class="c22">Motivation for Convolutional Neural Networks</span></li></ol><p class="c5 c19"><span class="c0"></span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; Fully connected networks aren&rsquo;t ideal for images because flattening destroys spatial relationships between nearby pixels, and the dense connections create too many parameters, making them costly and prone to overfitting. CNNs solve this by using local connectivity and shared weights, which preserve spatial patterns and make learning more efficient and effective for visual data.</span></p><p class="c5 c19"><span class="c0"></span></p><ol class="c33 lst-kix_list_13-0" start="2"><li class="c25 c36 li-bullet-0"><span class="c22">Digital Images as Inputs to CNNs</span></li></ol><p class="c5 c19"><span class="c6"></span></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp;A digital image can be represented as a 3D array with height, width, and channels. For color images, the three channels correspond to RGB intensity values. For example, CIFAR-10 images are 32 &times; 32 pixels with three channels, giving an input tensor of shape 32 &times; 32 &times; 3. CNNs take this multi-dimensional input directly, preserving spatial and channel-wise information during processing.</span></p><p class="c5 c19"><span class="c0"></span></p><ol class="c33 lst-kix_list_13-0" start="3"><li class="c25 c36 li-bullet-0"><span class="c11">Neurons in the Convolutional Layer</span></li></ol><p class="c10 c46"><span class="c11"></span></p><p class="c39"><span class="c0">There are neurons in convolution layers. Each neuron is connected only</span></p><p class="c39"><span class="c0">to a local region (receptive field) of the input, computes a weighted</span></p><p class="c39"><span class="c0">sum + bias, applies an activation function, and contributes to one</span></p><p class="c39"><span class="c0">element in a feature map.</span></p><p class="c39"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 456.00px; height: 360.00px;"><img alt="" src="images/image8.png" style="width: 456.00px; height: 360.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c0">Referring to above figure1 it shows us that :</span></p><ul class="c33 lst-kix_list_14-0 start"><li class="c3 li-bullet-0"><span class="c4">Left: The input image of size 5 &times; 5 is shown. The red box represents the </span><span class="c4 c43">receptive field</span><span class="c0">, i.e., a local 3 &times; 3 region of the image.<br></span></li><li class="c14 li-bullet-0"><span class="c0">Middle: A 3 &times; 3 filter (kernel) is applied to this local patch using element-wise multiplication and summation.<br></span></li><li class="c14 li-bullet-0"><span class="c0">Right: A single neuron output (one scalar value) is obtained, which forms one element of the feature map.<br></span></li><li class="c49 c38 c46 li-bullet-0"><span class="c0">Key idea: Each neuron in a convolutional layer is connected only to a local region of the input, not the entire image. This is known as local connectivity.</span></li></ul><p class="c10 c46"><span class="c0"></span></p><ol class="c33 lst-kix_list_13-0" start="4"><li class="c25 c36 li-bullet-0"><span class="c22">Convolution Operation</span></li></ol><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; &nbsp;Convolution slides a small filter over the image and computes weighted sums to capture local features, such as edges and textures. Reusing the same filter across the image reduces parameters while keeping strong feature learning.</span></p><p class="c53 c47 c63"><img src="images/image1.png"></p><p class="c5"><span class="c0">&nbsp; &nbsp; &nbsp; Mathematically, the convolution operation can be expressed as the above equation where I is the input image and K is the kernel.</span></p><p class="c5"><span class="c0">Refer fig 2. To understand how convolution operation is actually performed by the kernel on an input image matrix and a feature map is calculated.</span></p><p class="c2"><span class="c0">&nbsp;Refer fig 3 shown below to get an idea of how &nbsp;convolution operation is performed on &nbsp; &nbsp;RGB images which has 3 channels</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 520.00px; height: 332.00px;"><img alt="" src="images/image7.png" style="width: 520.00px; height: 332.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 523.67px; height: 387.83px;"><img alt="" src="images/image10.png" style="width: 523.67px; height: 387.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c19"><span class="c6"></span></p><ol class="c33 lst-kix_list_13-0" start="5"><li class="c25 c36 li-bullet-0"><span class="c22">Feature Maps and Hierarchical Feature Learning</span></li></ol><p class="c10 c58"><span class="c6"></span></p><p class="c25 c46"><span class="c0">After convolution, the output is a feature map. Each filter detects a specific feature, and combining multiple filters produces multiple feature maps. As layers go deeper, simple features (like edges) combine into complex ones (shapes and parts), letting CNNs learn useful patterns automatically without manual feature design.</span></p><p class="c10"><span class="c0"></span></p><ol class="c33 lst-kix_list_13-0" start="6"><li class="c2 c38 li-bullet-0"><span class="c22">&nbsp;Stride and Padding</span></li></ol><p class="c25 c31"><span class="c0">Stride is how many pixels the filter shifts each step; a larger stride reduces the output size. Padding adds (usually zero) pixels around the input to control output dimensions and keep edge information. where N is the input size, F filter size, P padding, and S stride.</span></p><p class="c53 c31"><img src="images/image2.png"></p><p class="c2 c47"><span class="c20 c28 c15">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Refer &nbsp;fig 4 to understand how stride operation is performed on input matrix with a kernel size of 3*3 to get feature map as 3*3</span></p><p class="c2 c47"><span class="c20">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Output=</span><span class="c62">&nbsp;</span><img src="images/image3.png"><span class="c20 c28 c15">+1 = 3</span></p><p class="c53 c54 c47"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 465.30px; height: 258.50px;"><img alt="" src="images/image9.png" style="width: 465.30px; height: 258.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 431.58px; height: 260.04px;"><img alt="" src="images/image12.png" style="width: 431.58px; height: 260.04px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c47"><span class="c20 c28 c15">Refer fig 5 to visualise how our output feature map will look like if padding is 1 so,</span></p><p class="c53 c54 c47"><span class="c20">Output= </span><img src="images/image4.png"><span class="c6">+1= 5</span></p><p class="c18 c54 c47"><span class="c20 c28 c15"></span></p><p class="c24"><span class="c11">VI. &nbsp; &nbsp; Activation Functions in CNNs</span></p><p class="c24 c19"><span class="c11"></span></p><p class="c25 c46"><span class="c4">After convolution, an activation function adds non-linearity. The most common is </span><img src="images/image5.png"><span class="c0">&nbsp; &nbsp;which helps reduce vanishing gradients and speeds up training. &nbsp;</span></p><p class="c53"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 298.12px; height: 239.50px;"><img alt="" src="images/image11.png" style="width: 298.12px; height: 239.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c18"><span class="c0"></span></p><p class="c25"><span class="c4">Refer figure &nbsp;shown below to understand how Relu function affects the filter output. &nbsp;</span></p><p class="c53 c47 c55"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 313.58px; height: 241.15px;"><img alt="" src="images/image14.png" style="width: 313.58px; height: 241.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c24 c19"><span class="c0"></span></p><p class="c25 c47"><span class="c11">VII. &nbsp; &nbsp; &nbsp;Pooling Layers</span></p><p class="c10"><span class="c11"></span></p><p class="c25"><span class="c0">Pooling downsamples feature maps to reduce computation and make features more &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; robust to small shifts. Common types are max pooling, average pooling, and global average pooling, and they can also help reduce overfitting.</span></p><p class="c25"><span class="c11">VII . i Max Pooling</span></p><p class="c25"><span class="c0">Max pooling is a pooling operation that selects the maximum element from the region of the feature map covered by the filter. Thus, the output after max-pooling layer would be a feature map containing the most prominent features of the previous feature map as shown in Fig 7.</span></p><p class="c25"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 477.67px; height: 317.54px;"><img alt="" src="images/image13.png" style="width: 477.67px; height: 317.54px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c12"></span></p><p class="c10"><span class="c0"></span></p><p class="c25"><span class="c11">ii Average Pooling</span></p><p class="c25"><span class="c0">Average pooling computes the average of the elements present in the region of the feature &nbsp; map covered by the filter.Thus, while max pooling gives the most prominent feature in a particular patch of the feature map, average pooling gives the average of features present in a patch as shown in Fig 8.</span></p><p class="c10"><span class="c0"></span></p><p class="c25"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 462.67px; height: 306.36px;"><img alt="" src="images/image16.png" style="width: 462.67px; height: 306.36px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c0"></span></p><p class="c10 c46 c61"><span class="c0"></span></p><ol class="c33 lst-kix_list_13-0" start="7"><li class="c25 c38 c46 li-bullet-0"><span class="c22">Flattening</span></li></ol><p class="c1 c42"><span class="c22">&nbsp; &nbsp; </span><span class="c0">Before entering the fully connected layer, the featuremaps from the previous convolutional and pooling layers are typically flattened into a one-dimensional vector as shown in Fig 9.This is done to convert the spatial information into a format suitable for fully connected layers.</span></p><p class="c10"><span class="c0"></span></p><p class="c25"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 524.00px; height: 263.00px;"><img alt="" src="images/image15.png" style="width: 524.00px; height: 263.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c0"></span></p><p class="c25 c42"><span class="c11">IX. &nbsp; &nbsp;Overall CNN Architecture</span></p><p class="c10 c42"><span class="c11"></span></p><p class="c2 c48"><span class="c0">&nbsp; &nbsp; &nbsp; A typical CNN stacks convolution, activation, and pooling layers to extract features, then uses fully connected layers or global pooling as a classifier as shown in FIg 10. This pipeline enables strong image recognition performance with efficient use of parameters.</span></p><p class="c10 c48 c54"><span class="c6"></span></p><p class="c10 c47"><span class="c6"></span></p><p class="c25 c47"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 597.17px; height: 402.54px;"><img alt="" src="images/image6.png" style="width: 597.17px; height: 402.54px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10 c47"><span class="c6"></span></p><p class="c25"><span class="c22 c44">Merits of Convolutional Neural Networks:</span></p><ul class="c33 lst-kix_list_1-0 start"><li class="c7 li-bullet-1"><span class="c0">Efficient parameter sharing (fewer weights than fully connected networks)</span></li><li class="c25 c34 li-bullet-2"><span class="c0">Preserves spatial structure in images</span></li><li class="c25 c34 li-bullet-2"><span class="c0">Automatically learns and extracts features</span></li><li class="c25 c34 li-bullet-1"><span class="c0">Strong generalization on image data</span></li><li class="c25 c34 li-bullet-2"><span class="c0">Scalable to large and complex visual tasks</span></li><li class="c41 c34 li-bullet-1"><span class="c0">Backbone of modern computer vision systems</span></li></ul><p class="c25"><span class="c44 c22">Demerits of Convolutional Neural Networks:</span></p><ul class="c33 lst-kix_list_4-0 start"><li class="c7 li-bullet-2"><span class="c0">Require high computational resources (GPU/TPU)</span></li><li class="c25 c34 li-bullet-2"><span class="c0">Need large labeled datasets for best performance</span></li><li class="c25 c34 li-bullet-1"><span class="c0">Training deep CNNs can be time-consuming</span></li><li class="c41 c34 li-bullet-1"><span class="c0">Features learned are hard to interpret (low explainability)</span></li></ul><p class="c10"><span class="c6"></span></p><p class="c25"><span class="c22">3. Pre-Test (MCQs)</span></p><ol class="c33 lst-kix_list_5-0 start" start="1"><li class="c37 li-bullet-3"><span class="c12">What type of data are Convolutional Neural Networks primarily designed to process?</span></li></ol><p class="c2"><span class="c0">a) Sequential data</span></p><p class="c2"><span class="c0">(Incorrect because sequential data, such as text or time series, are better handled by RNNs or Transformers.)</span></p><p class="c2"><span class="c0">b) Tabular data</span></p><p class="c2"><span class="c0">(Incorrect because tabular data does not inherently contain spatial structure.)</span></p><p class="c2"><span class="c0">c) Image data</span></p><p class="c2"><span class="c0">(Correct because CNNs are specifically designed to exploit spatial relationships present in images.)</span></p><p class="c2"><span class="c0">d) Categorical data</span></p><p class="c2"><span class="c0">(Incorrect because categorical data requires encoding techniques rather than convolution operations.)</span></p><p class="c2"><span class="c17">Answer: </span><span class="c0">c</span></p><ol class="c33 lst-kix_list_5-0" start="2"><li class="c8 c45 li-bullet-4"><h3 style="display:inline"><span class="c4 c15">Why are fully connected neural networks inefficient for image classification tasks?</span></h3></li></ol><p class="c2"><span class="c0">a) They cannot use activation functions</span></p><p class="c2"><span class="c0">(Incorrect because fully connected networks can use activation functions.)</span></p><p class="c2"><span class="c0">b) They ignore spatial relationships between pixels</span></p><p class="c2"><span class="c0">(Correct because flattening images destroys spatial locality information.)</span></p><p class="c2"><span class="c0">c) They cannot be trained using gradient descent</span></p><p class="c2"><span class="c0">(Incorrect because fully connected networks are trainable using gradient-based methods.)</span></p><p class="c2"><span class="c0">d) They require labeled data</span></p><p class="c2"><span class="c0">(Incorrect because labeled data is required by most supervised learning models.)</span></p><p class="c2"><span class="c17">Answer: </span><span class="c0">b</span></p><ol class="c33 lst-kix_list_5-0" start="3"><li class="c8 c56 li-bullet-5"><h3 style="display:inline"><span class="c4 c15">In an RGB image, how many channels are present?</span></h3></li></ol><p class="c2"><span class="c0">a) One</span></p><p class="c2"><span class="c0">(Incorrect because grayscale images have one channel, not RGB images.)</span></p><p class="c2"><span class="c0">b) Two</span></p><p class="c2"><span class="c0">(Incorrect because RGB consists of three color components.)</span></p><p class="c2"><span class="c0">c) Three</span></p><p class="c2"><span class="c0">(Correct because RGB images contain Red, Green, and Blue channels.)</span></p><p class="c2"><span class="c0">d) Four</span></p><p class="c2"><span class="c0">(Incorrect because an alpha channel is not part of standard RGB images.)</span></p><p class="c2"><span class="c17">Answer: </span><span class="c0">c</span></p><h3 class="c8"><span class="c4 c32">4. &nbsp; &nbsp;</span><span class="c4 c15">What is the primary purpose of a convolution filter in a CNN?</span></h3><p class="c2"><span class="c0">a) To reduce the number of training samples</span></p><p class="c2"><span class="c0">(Incorrect because filters operate on features, not on sample count.)</span></p><p class="c2"><span class="c0">b) To detect local patterns in the input image</span></p><p class="c2"><span class="c0">(Correct because filters learn to identify features such as edges and textures.)</span></p><p class="c2"><span class="c0">c) To normalize pixel values</span></p><p class="c2"><span class="c0">(Incorrect because normalization is a preprocessing step.)</span></p><p class="c2"><span class="c0">d) To perform classification directly</span></p><p class="c2"><span class="c0">(Incorrect because classification is performed by the final layers.)</span></p><p class="c2"><span class="c17">Answer: </span><span class="c0">b</span></p><h3 class="c8"><span class="c4 c15">5. &nbsp; What is the output produced after applying a convolution operation called?</span></h3><p class="c16"><span class="c0">a) Kernel</span></p><p class="c16"><span class="c0">(Incorrect because a kernel is the filter applied, not the output.)</span></p><p class="c16"><span class="c0">b) Feature map</span></p><p class="c16"><span class="c0">(Correct because the convolution operation produces feature maps.)</span></p><p class="c16"><span class="c0">c) Weight matrix</span></p><p class="c16"><span class="c0">(Incorrect because weights belong to the filter.)</span></p><p class="c16"><span class="c0">d) Loss map</span></p><p class="c16"><span class="c0">(Incorrect because loss is computed after prediction.)</span></p><p class="c16"><span class="c17">Answer:</span><span class="c0">&nbsp;b</span></p><h3 class="c8"><span class="c4 c15">6. &nbsp;Why is the ReLU activation function commonly used in CNNs?</span></h3><p class="c16"><span class="c0">a) It increases computational complexity</span></p><p class="c16"><span class="c0">(Incorrect because ReLU actually reduces computational cost.)</span></p><p class="c16"><span class="c0">b) It introduces non-linearity into the model</span></p><p class="c16"><span class="c0">(Correct because non-linearity allows learning complex patterns.)</span></p><p class="c16"><span class="c0">c) It converts images into vectors</span></p><p class="c16"><span class="c0">(Incorrect because flattening performs this operation.)</span></p><p class="c16"><span class="c0">d) It performs pooling</span></p><p class="c16"><span class="c0">(Incorrect because pooling is a separate operation.)</span></p><p class="c16"><span class="c17">Answer:</span><span class="c0">&nbsp;b</span></p><h3 class="c8"><span class="c4 c15">7. &nbsp;What is the main purpose of pooling layers in CNNs?</span></h3><p class="c1"><span class="c0">a) To increase the spatial dimensions of feature maps</span></p><p class="c1"><span class="c0">(Incorrect because pooling reduces spatial dimensions.)</span></p><p class="c1"><span class="c0">b) To reduce computational complexity</span></p><p class="c1"><span class="c0">(Correct because pooling downsamples feature maps.)</span></p><p class="c1"><span class="c0">c) To add non-linearity</span></p><p class="c1"><span class="c0">(Incorrect because activation functions provide non-linearity.)</span></p><p class="c1"><span class="c0">d) To classify images</span></p><p class="c1"><span class="c0">(Incorrect because classification is done by final layers.)</span></p><p class="c1"><span class="c17">Answer: </span><span class="c0">b</span></p><h3 class="c8"><span class="c4 c15">8. &nbsp; Which pooling technique selects the maximum value from a region?</span></h3><p class="c1"><span class="c0">a) Average Pooling</span></p><p class="c1"><span class="c0">(Incorrect because it computes the mean value.)</span></p><p class="c1"><span class="c0">b) Min Pooling</span></p><p class="c1"><span class="c0">(Incorrect because it selects the minimum value.)</span></p><p class="c1"><span class="c0">c) Max Pooling</span></p><p class="c1"><span class="c0">(Correct because it selects the highest activation in a region.)</span></p><p class="c1"><span class="c0">d) Global Pooling</span></p><p class="c1"><span class="c0">(Incorrect because it operates over the entire feature map.)</span></p><p class="c1"><span class="c17">Answer: </span><span class="c0">c</span></p><h3 class="c8"><span class="c4 c15">9. &nbsp;What advantage does parameter sharing provide in CNNs?</span></h3><p class="c16"><span class="c0">a) Increases the number of parameters</span></p><p class="c16"><span class="c0">(Incorrect because parameter sharing reduces parameters.)</span></p><p class="c16"><span class="c0">b) Reduces overfitting and memory usage</span></p><p class="c16"><span class="c0">(Correct because fewer parameters improve generalization.)</span></p><p class="c16"><span class="c0">c) Eliminates the need for training</span></p><p class="c16"><span class="c0">(Incorrect because training is still required.)</span></p><p class="c16"><span class="c0">d) Removes the need for activation functions</span></p><p class="c16"><span class="c0">(Incorrect because activation functions are essential.)</span></p><p class="c16"><span class="c17">Answer: </span><span class="c0">b</span></p><h3 class="c8"><span class="c4 c15">10. Which task is CNNs most commonly used for?</span></h3><p class="c1"><span class="c0">a) Sorting numerical data</span></p><p class="c1"><span class="c0">(Incorrect because sorting is not a learning task.)</span></p><p class="c1"><span class="c0">b) Image classification</span></p><p class="c1"><span class="c0">(Correct because CNNs excel at visual recognition tasks.)</span></p><p class="c1"><span class="c0">c) Database management</span></p><p class="c1"><span class="c0">(Incorrect because databases are not learning models.)</span></p><p class="c1"><span class="c0">d) Text summarization</span></p><p class="c1"><span class="c0">(Incorrect because NLP tasks require different architectures.)</span></p><p class="c1"><span class="c17">Answer: </span><span class="c0">b</span></p><p class="c10"><span class="c6"></span></p><ol class="c33 lst-kix_list_5-0" start="4"><li class="c25 c50 li-bullet-5"><span class="c22">Procedure</span></li></ol><p class="c25"><span class="c0">The objective of this experiment is to implement a Convolutional Neural Networks (CNN) for multi-class image classification and to analyze its performance on a real-world colour image dataset. This experiment focuses on understanding convolutional feature extraction, pooling operations, data augmentation, training dynamics, and evaluation strategies using the CIFAR-10 dataset.</span></p><p class="c25"><span class="c4">1.</span><span class="c12">Import Required Libraries</span></p><ul class="c33 lst-kix_list_6-0 start"><li class="c7 li-bullet-2"><span class="c0">PyTorch (torch, torch.nn, torch.): build the CNN, define loss, and train the model.</span></li><li class="c25 c34 li-bullet-2"><span class="c0">Torchvision (torchvision.datasets, torchvision.transforms): load CIFAR-10 and apply image preprocessing/augmentation.</span></li><li class="c25 c34 li-bullet-1"><span class="c0">NumPy (numpy): basic numerical utilities (optional; PyTorch often enough).</span></li><li class="c41 c34 li-bullet-1"><span class="c0">Matplotlib (matplotlib.pyplot): visualize sample images and training/validation graphs.</span></li></ul><p class="c25"><span class="c4">2.</span><span class="c12">Dataset Loading and Description</span></p><ul class="c33 lst-kix_list_7-0 start"><li class="c7 li-bullet-1"><span class="c4">Load CIFAR-10 with </span><span class="c4 c60">t</span><span class="c0">orchvision.datasets.CIFAR10</span></li><li class="c25 c34 li-bullet-2"><span class="c0">Dataset info (important):</span></li></ul><ul class="c33 lst-kix_list_7-1 start"><li class="c25 c30 li-bullet-6"><span class="c4">Total: 60,000 RGB images, size 32&times;32</span></li><li class="c25 c30 li-bullet-7"><span class="c4">Split: 50,000 train + 10,000 test</span></li><li class="c41 c30 li-bullet-8"><span class="c4">Classes: 10 categories (e.g., airplane, automobile, bird, cat, etc.)</span></li></ul><p class="c25"><span class="c4">3.</span><span class="c12">Exploratory Data Analysis (EDA)</span></p><ul class="c33 lst-kix_list_8-0 start"><li class="c7 li-bullet-1"><span class="c0">Visualize: 1 sample image per class (10 classes) in a labeled grid.</span></li><li class="c25 c34 li-bullet-1"><span class="c0">Verify: image size 32&times;32, RGB channels.</span></li><li class="c41 c34 li-bullet-9"><span class="c0">Inspect: class diversity, background variation, and ambiguity between similar classes.</span></li></ul><p class="c25"><span class="c4">4.</span><span class="c12">Data Preprocessing</span></p><ul class="c33 lst-kix_list_9-0 start"><li class="c7 li-bullet-1"><span class="c0">a) Tensor conversion</span></li><li class="c25 c34 li-bullet-1"><span class="c0">Convert each image to a PyTorch tensor of shape (C, H, W) with values in [0, 1].</span></li><li class="c25 c34 li-bullet-9"><span class="c0">b) Normalization (CIFAR-10)</span></li><li class="c25 c34 li-bullet-1"><span class="c0">Normalize per channel to stabilize training:</span></li></ul><ul class="c33 lst-kix_list_9-1 start"><li class="c25 c30 li-bullet-6"><span class="c0">Mean = (0.4914, 0.4822, 0.4465)</span></li><li class="c41 c30 li-bullet-7"><span class="c0">Std = (0.2470, 0.2435, 0.2616)</span></li></ul><p class="c10"><span class="c0"></span></p><p class="c25"><span class="c4">5.</span><span class="c12">Data loading and augmentation</span></p><ul class="c33 lst-kix_list_10-0 start"><li class="c7 li-bullet-1"><span class="c0">Perform Augmentation and preprocess (train only): random horizontal flip, random crop with padding, (optional) AutoAugment which in tends increases diversity, reduces overfitting, improves generalization.</span></li><li class="c25 c34 li-bullet-1"><span class="c0">Create DataLoaders (train &amp; test): load data in mini-batches for faster, GPU-friendly training and evaluation.</span></li><li class="c41 c34 li-bullet-1"><span class="c0">Shuffling rule: shuffle training loader (avoid order bias), no shuffle for test loader (consistent evaluation).</span></li></ul><p class="c25"><span class="c4">6.</span><span class="c12">CNN Model Architecture Design</span></p><ul class="c33 lst-kix_list_15-0 start"><li class="c3 li-bullet-0"><span class="c4">Input (32&times;32&times;3)</span></li><li class="c14 li-bullet-0"><span class="c27">(Conv 2d&rarr; BatchNorm2d &rarr; ReLU])&times; N (extract features)</span></li><li class="c14 li-bullet-0"><span class="c4">Dropout2d (0.05) This help in reducing overfitting</span></li><li class="c14 li-bullet-0"><span class="c4">MaxPool (periodically) (reduce H&times;W, keep important patterns)</span></li><li class="c38 c46 c49 li-bullet-0"><span class="c27">Global Average Pooling (convert feature maps &rarr; single feature vector)</span></li></ul><p class="c10 c46"><span class="c0"></span></p><p class="c25"><span class="c4">7. </span><span class="c12">Training Configuration </span></p><ul class="c33 lst-kix_list_11-0 start"><li class="c7 li-bullet-1"><span class="c0">Loss: CrossEntropyLoss (for 10-class classification)</span></li><li class="c25 c34 li-bullet-9"><span class="c0">Adam Optimizer with Regularization: use SGD/Adam with weight decay (L2) and dropout &nbsp;of (0.05) to reduce overfitting</span></li><li class="c41 c34 li-bullet-1"><span class="c0">LR Scheduler: adjust learning rate during training (e.g., StepLR / CosineAnnealing) for better convergence</span></li></ul><p class="c25"><span class="c4">8. </span><span class="c12">Model Training</span></p><p class="c35"><span class="c0">The model is trained for a fixed number of epochs, where in each epoch a forward pass is performed to generate predictions, the loss is computed, and backpropagation is applied followed by an optimizer step to update the model parameters. During this process, the training loss and accuracy are recorded at the end of every epoch to monitor learning progress.</span></p><p class="c10"><span class="c12"></span></p><p class="c25"><span class="c4">9. </span><span class="c12">Model Evaluation</span></p><p class="c25"><span class="c0">Test the trained CNN on unseen data by computing test accuracy and error metrics (e.g., confusion matrix/class-wise accuracy) to identify where predictions fail.</span></p><p class="c10"><span class="c0"></span></p><p class="c25"><span class="c4">10.</span><span class="c12">&nbsp;Result Visualization</span></p><ul class="c33 lst-kix_list_2-0 start"><li class="c7 li-bullet-2"><span class="c0">Plot train vs test curves for loss and accuracy to check convergence/overfitting.</span></li><li class="c34 c41 li-bullet-2"><span class="c0">Show a few sample predictions with true label &nbsp;predicted label (optionally confidence) for qualitative evaluation.</span></li></ul><p class="c25"><span class="c4">11.</span><span class="c12">&nbsp;Performance Analysis</span></p><ul class="c33 lst-kix_list_3-0 start"><li class="c7 li-bullet-9"><span class="c0">Per Class-accuracy: identify best/worst performing classes.</span></li><li class="c41 c34 li-bullet-1"><span class="c0">Confusion matrix: highlight frequently confused class pairs and error patterns.</span></li></ul><p class="c10"><span class="c6"></span></p><p class="c25 c29"><span class="c22">5. Post-Test (MCQs)</span></p><h3 class="c8"><span class="c4 c15">1. &nbsp; Why is data augmentation used while training a CNN on the CIFAR-10 dataset?</span></h3><p class="c1"><span class="c0">a) To increase the number of test samples</span></p><p class="c1"><span class="c0">(Incorrect because augmentation is applied only to the training data, not the test data.)</span></p><p class="c1"><span class="c0">b) To create diverse variations of training images</span></p><p class="c1"><span class="c0">(Correct because augmentation generates modified versions of existing images, improving generalization.)</span></p><p class="c1"><span class="c0">c) To reduce the number of classes</span></p><p class="c1"><span class="c0">(Incorrect because augmentation does not change the number of classes.)</span></p><p class="c1"><span class="c0">d) To replace normalization</span></p><p class="c1"><span class="c0">(Incorrect because augmentation and normalization serve different purposes.)</span></p><p class="c1"><span class="c17">Answer: </span><span class="c0">b</span></p><h3 class="c8"><span class="c4 c15">2. What is the main advantage of using convolutional layers instead of fully </span><span class="c4 c32">connected layers</span><span class="c4 c15">&nbsp;for images?</span></h3><p class="c16"><span class="c0">a) They require more parameters</span></p><p class="c16"><span class="c0">(Incorrect because convolutional layers significantly reduce the number of parameters.)</span></p><p class="c16"><span class="c0">b) They preserve spatial relationships between pixels</span></p><p class="c16"><span class="c0">(Correct because convolution operates on local neighborhoods and maintains spatial &nbsp; structure.)</span></p><p class="c16"><span class="c0">c) They eliminate the need for training</span></p><p class="c16"><span class="c0">(Incorrect because CNNs still require training through backpropagation.)</span></p><p class="c16"><span class="c0">d) They directly perform classification</span></p><p class="c16"><span class="c0">(Incorrect because classification is done by the final layers.)</span></p><p class="c16"><span class="c17">Answer:</span><span class="c0">&nbsp;b</span></p><h3 class="c8"><span class="c4 c15">3. Why is normalization applied to CIFAR-10 images before training the CNN?</span></h3><p class="c16"><span class="c0">a) To convert images into grayscale</span></p><p class="c16"><span class="c0">(Incorrect because normalization does not change the number of channels.)</span></p><p class="c16"><span class="c0">b) To scale pixel values for stable and faster training</span></p><p class="c16"><span class="c0">(Correct because normalized inputs improve gradient stability and convergence.)</span></p><p class="c16"><span class="c0">c) To reduce the dataset size</span></p><p class="c16"><span class="c0">(Incorrect because normalization does not affect the dataset size.)</span></p><p class="c16"><span class="c0">d) To remove noise from images</span></p><p class="c16"><span class="c0">(Incorrect because normalization does not perform denoising.)</span></p><p class="c16"><span class="c17">Answer: </span><span class="c0">b</span></p><h3 class="c8"><span class="c4 c15">4. What is the purpose of pooling layers in a CNN architecture?</span></h3><p class="c1"><span class="c0">a) To increase spatial resolution</span></p><p class="c1"><span class="c0">(Incorrect because pooling reduces spatial dimensions.)</span></p><p class="c1"><span class="c0">b) To reduce computational complexity and overfitting</span></p><p class="c1"><span class="c0">(Correct because pooling downsamples feature maps and reduces parameters.)</span></p><p class="c1"><span class="c0">c) To introduce non-linearity</span></p><p class="c1"><span class="c0">(Incorrect because activation functions provide non-linearity.)</span></p><p class="c1"><span class="c0">d) To classify images</span></p><p class="c1"><span class="c0">(Incorrect because classification is handled by the final layer.)</span></p><p class="c1"><span class="c17">Answer: </span><span class="c0">b</span></p><ol class="c33 lst-kix_list_5-0" start="5"><li class="c25 c56 li-bullet-4"><span class="c17">Why is ReLU commonly used as an activation function in CNNs?</span></li></ol><p class="c16"><span class="c0">a) It converts outputs into probabilities</span></p><p class="c16"><span class="c0">(Incorrect because softmax performs probability conversion.)</span></p><p class="c16"><span class="c0">b) It helps mitigate the vanishing gradient problem</span></p><p class="c16"><span class="c0">(Correct because ReLU maintains strong gradients for positive activations.)</span></p><p class="c16"><span class="c0">c) It performs feature extraction</span></p><p class="c16"><span class="c0">(Incorrect because convolution extracts features, not activation functions.)</span></p><p class="c16"><span class="c0">d) It replaces pooling layers</span></p><p class="c16"><span class="c0">(Incorrect because activation and pooling are separate operations.)</span></p><p class="c16"><span class="c17">Answer:</span><span class="c0">&nbsp;b</span></p><h3 class="c8"><span class="c4 c15">6. &nbsp; What role does Global Average Pooling play in the CNN model used in this experiment?</span></h3><p class="c1"><span class="c0">a) It increases the number of trainable parameters</span></p><p class="c1"><span class="c0">(Incorrect because it reduces parameters.)</span></p><p class="c1"><span class="c0">b) It aggregates spatial information before classification</span></p><p class="c1"><span class="c0">(Correct because it summarizes each feature map into a single value.)</span></p><p class="c1"><span class="c0">c) It replaces convolution layers</span></p><p class="c1"><span class="c0">(Incorrect because it operates after feature extraction.)</span></p><p class="c1"><span class="c0">d) It performs data augmentation</span></p><p class="c1"><span class="c0">(Incorrect because augmentation is a preprocessing step.)</span></p><p class="c1"><span class="c17">Answer: </span><span class="c0">b</span></p><h3 class="c8"><span class="c4 c15">7. &nbsp;Why is the CIFAR-10 dataset considered challenging for CNN models?</span></h3><p class="c1"><span class="c0">a) Because images are very large</span></p><p class="c1"><span class="c0">(Incorrect because CIFAR-10 images are small.)</span></p><p class="c1"><span class="c0">b) Because images have no labels</span></p><p class="c1"><span class="c0">(Incorrect because CIFAR-10 is a labeled dataset.)</span></p><p class="c1"><span class="c0">c) Because images are low resolution with high inter-class similarity</span></p><p class="c1"><span class="c0">(Correct because small images and similar classes increase classification difficulty.)</span></p><p class="c1"><span class="c0">d) Because it contains only grayscale images</span></p><p class="c1"><span class="c0">(Incorrect because CIFAR-10 contains RGB images.)</span></p><p class="c1"><span class="c17">Answer:</span><span class="c0">&nbsp;c</span></p><h3 class="c8"><span class="c4 c15">8. What information does a confusion matrix provide in CNN evaluation?</span></h3><p class="c1"><span class="c0">a) Training speed of the model</span></p><p class="c1"><span class="c0">(Incorrect because speed is not shown in a confusion matrix.)</span></p><p class="c1"><span class="c0">b) Correct and incorrect class-wise predictions</span></p><p class="c1"><span class="c0">(Correct because it shows how predictions are distributed across classes.)</span></p><p class="c1"><span class="c0">c) Loss function values</span></p><p class="c1"><span class="c0">(Incorrect because loss is not displayed in a confusion matrix.)</span></p><p class="c1"><span class="c0">d) Learning rate changes</span></p><p class="c1"><span class="c0">(Incorrect because the learning rate is not represented.)</span></p><p class="c1"><span class="c17">Answer: </span><span class="c0">b</span></p><h3 class="c8"><span class="c4 c15">9. Why are training and testing accuracy curves plotted after CNN training?</span></h3><p class="c1"><span class="c0">a) To increase model accuracy</span></p><p class="c1"><span class="c0">(Incorrect because plotting does not affect model performance.)</span></p><p class="c1"><span class="c0">b) To visualize learning behavior and detect overfitting</span></p><p class="c1"><span class="c0">(Correct because divergence between curves indicates overfitting or underfitting.)</span></p><p class="c1"><span class="c0">c) To perform data preprocessing</span></p><p class="c1"><span class="c0">(Incorrect because preprocessing occurs before training.)</span></p><p class="c1"><span class="c0">d) To reduce training time</span></p><p class="c1"><span class="c0">(Incorrect because visualization does not speed up training.)</span></p><p class="c1"><span class="c17">Answer:</span><span class="c0">&nbsp;b</span></p><h3 class="c8"><span class="c4 c15">10. Which factor most strongly contributes </span><span class="c4 c32">to CNN&#39;s</span><span class="c4 c15">&nbsp;ability to generalize well in this experiment?</span></h3><p class="c16"><span class="c0">a) Increasing the number of classes</span></p><p class="c16"><span class="c0">(Incorrect because generalization is not improved by more classes.)</span></p><p class="c16"><span class="c0">b) Using data augmentation and regularization techniques</span></p><p class="c16"><span class="c0">(Correct because these techniques reduce overfitting and improve robustness.)</span></p><p class="c16"><span class="c0">c) Removing pooling layers</span></p><p class="c16"><span class="c0">(Incorrect because pooling helps control overfitting.)</span></p><p class="c16"><span class="c0">d) Avoiding normalization</span></p><p class="c16"><span class="c0">(Incorrect because normalization improves training stability.)</span></p><p class="c16"><span class="c17">Answer:</span><span class="c0">&nbsp;b</span></p><p class="c10"><span class="c0"></span></p><p class="c25"><span class="c22">6. References</span></p><ol class="c33 lst-kix_list_12-0 start" start="1"><li class="c21 li-bullet-0"><span class="c4">Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, &ldquo;Gradient-based learning applied to document recognition,&rdquo; </span><span class="c4 c43">Proceedings of the IEEE</span><span class="c4">, vol. 86, no. 11, pp. 2278&ndash;2324, 1998.</span></li><li class="c23 li-bullet-0"><span class="c4">Dataset Link:https://www.kaggle.com/datasets/ayush1220/cifar10?</span></li><li class="c23 li-bullet-0"><span class="c4">(Original reference for the CIFAR-10 dataset used in this experiment.)</span></li><li class="c38 c46 c51 li-bullet-0"><span class="c4">I Goodfellow, Y. Bengio, and A. Courville, </span><span class="c4 c43">Deep Learning</span><span class="c4">, MIT Press, 2016.</span></li></ol><p class="c10"><span class="c6"></span></p><p class="c10"><span class="c6"></span></p><p class="c10"><span class="c6"></span></p></body></html>